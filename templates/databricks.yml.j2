# --- PLANTILLA YML.J2 ---
# Usa 'sync.include:' y la ruta de tu workspace

bundle:
  name: "framework-dlt-generado"

# Ruta absoluta del proyecto en Databricks Repos
workspace:
  root_path: "/Workspace/Users/ignaqwert00@gmail.com/prueba_tecnica_etl"

# Usamos 'sync.include' para SINCRONIZAR ficheros .py
sync:
  include:
    - "src/dlt_helpers.py"
    - "pipelines_generadas/*.py" # Incluye todos los .py generados

targets:
  dev:
    mode: development
    default: true
    workspace: {} # Limpio y vac√≠o
    
    resources:
      pipelines:
        {% for df in dataflows %}
        dlt_{{ df.name }}:
          name: "DLT - {{ df.name }}"
          #development: true
          serverless: true
          libraries:
            # El path es ahora relativo al root_path de arriba
            - notebook: { path: "pipelines_generadas/dlt_{{ df.name }}.py" }
          #storage: "/Volumes/workspace/elt_modular_dlt/dlt_storage/{{ df.name }}"
          catalog: "workspace"
          schema: "elt_modular_dlt"
          notifications:
            - email_recipients: [ "ignaqwert00@gmail.com" ]
              alerts: [ "on_failure" ]
        {% endfor %}

      jobs:
        mega_job_all_pipelines:
          name: "JOB Orquestador - Todos los Dataflows"
          tasks:
            # Bucle FOR para crear una tarea POR dataflow
            {% for df in dataflows %}
            - task_key: "run_dlt_{{ df.name }}"
              description: "Ejecuta el pipeline DLT para {{ df.name }}"
              pipeline_task:
                pipeline_id: "${resources.pipelines.dlt_{{ df.name }}.id}"
              # No hay 'depends_on', todo se ejecuta en paralelo
            {% endfor %}