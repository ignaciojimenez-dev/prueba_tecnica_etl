# Databricks notebook source
# --- GENERADO AUTOMÁTICAMENTE POR JINJA2 ---
# Dataflow: {{ dataflow.name }}
# ------------------------------------------

import pyspark.pipelines as dp
from pyspark.sql import functions as F
from pyspark.sql import SparkSession

# Importamos los módulos de lógica reutilizable
from src import dlt_helpers

# --- 1. CAPA BRONCE (GENERADA DESDE 'sources') ---
{% for source in dataflow.sources %}
{% set bronze_table_name = dataflow.bronze_sinks | selectattr('input', 'equalto', source.name) | map(attribute='name') | first %}
@dp.table(
    name="{{ bronze_table_name }}",
    comment="Carga incremental (Autoloader) desde {{ source.path }}"
)
def {{ bronze_table_name }}():
    """ Tabla Bronze para {{ source.name }} """
    return (
        spark.readStream.format("cloudFiles")
            .option("cloudFiles.format", "{{ source.format }}")
            .option("cloudFiles.inferColumnTypes", "true")
            .option("ignoreMissingFiles", "true")
            .load("{{ source.path }}")
    )

{% endfor %}
# --- 2. CAPA PLATA (GENERADA DESDE 'transformations_silver') ---

{# Iteramos sobre las transformaciones de validacion #}
{% for tx in dataflow.transformations_silver if tx.type == 'validate_fields' %}
{% set input_table = tx.params.input %}
{% set validation_name = tx.name %}
{% set pre_quality_table_name = "silver_pre_quality_" + input_table %}

@dp.table(
    name="{{ pre_quality_table_name }}",
    comment="Aplica reglas de calidad DLT a la tabla {{ input_table }}",
)
# --- Usamos 'expect_all_or_drop' y el helper ---
@dp.expect_all_or_drop(dlt_helpers.generate_validation_rules({{ tx.params.validations }}))
def {{ pre_quality_table_name }}():
    """ Aplica expectativas y descarta registros malos de {{ input_table }} """
    return dp.read_stream("{{ input_table }}")

{#  transformación 'add_fields' para 'ok' #}
{% set ok_tx_input = validation_name + '_ok' %}
{% set final_ok_tx = (dataflow.transformations_silver | selectattr('params.input', 'equalto', ok_tx_input) | first) %}
{% set final_ok_sink = (dataflow.silver_sinks | selectattr('input', 'equalto', final_ok_tx.name) | first) %}

@dp.table(
    name="{{ final_ok_sink.name }}",
    comment="Registros OK de {{ input_table }}, enriquecidos."
)
def {{ final_ok_sink.name }}():
    """ 
    Lee los registros que pasaron la calidad de {{ pre_quality_table_name }}
    y aplica transformaciones finales.
    """
    # --- sin filter quarentine , mirar si se puede aplicar ---
    df_ok = dp.read_stream("{{ pre_quality_table_name }}")
    
    # Usamos la nueva función genérica 'apply_transformations'
    return dlt_helpers.apply_transformations(
        df_ok,
        [{{ final_ok_tx | tojson }}]
    )
{# 
    Al usar 'expect_all_or_drop', los registros malos se descartan 
    y no pueden ser capturados en una tabla de KO.
#}

{% endfor %}


# --- 3. CAPA ORO (GENERADA DESDE 'transformations_gold') ---

{# Iteramos sobre las transformaciones de ORO #}
{% for tx in dataflow.transformations_gold %}
{% set input_table = tx.params.input %} {# ej: "silver_person_ok" #}
{% set transform_name = tx.name %} {# ej: "mask_person_pii" masking#}
{% set gold_sink = (dataflow.gold_sinks | selectattr('input', 'equalto', transform_name) | first) %}

@dp.table(
    name="{{ gold_sink.name }}", {# ej: "gold_persons_anonymized" #}
    comment="Capa Oro: {{ transform_name }}"
)
def {{ gold_sink.name }}():
    """
    Lee desde la capa Plata ({{ input_table }}) 
    y aplica transformaciones de Oro ({{ transform_name }}).
    """
    df_silver = dp.read_stream("{{ input_table }}")
    
    # ¡Reutilizamos la MISMA función helper genérica!
    # Pasamos la configuración de 'apply_masking'
    return dlt_helpers.apply_transformations(
        df_silver,
        [{{ tx | tojson }}]
    )

{% endfor %}