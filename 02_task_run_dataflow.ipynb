{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5102962f",
   "metadata": {},
   "source": [
    "Tarea 2 For-each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac8dab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: 02_task_run_dataflow\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# IMPORTANTE: Asume que tu código 'src' está disponible\n",
    "# (ej. porque estás usando Databricks Repos)\n",
    "from src import orchestrator\n",
    "from src import utils\n",
    "\n",
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. Definir el Widget de Entrada ---\n",
    "# Este widget recibirá la configuración de UN solo dataflow.\n",
    "# El Job \"For-Each\" de Databricks rellenará este widget.\n",
    "# IMPORTANTE: Databricks pasa los diccionarios como un string de JSON.\n",
    "dbutils.widgets.text(\"dataflow_config_json\", \"{}\", \"Configuración del Dataflow (JSON String)\")\n",
    "\n",
    "# --- 2. Leer el Widget ---\n",
    "dataflow_json_string = dbutils.widgets.get(\"dataflow_config_json\")\n",
    "log.info(\"Tarea 2 (For-Each) iniciada.\")\n",
    "\n",
    "try:\n",
    "    # --- 3. Convertir el string JSON de vuelta a un diccionario ---\n",
    "    dataflow_config = json.loads(dataflow_json_string)\n",
    "    \n",
    "    if not dataflow_config or 'name' not in dataflow_config:\n",
    "        raise ValueError(\"Configuración de dataflow vacía o inválida recibida.\")\n",
    "        \n",
    "    dataflow_name = dataflow_config.get(\"name\")\n",
    "    log.info(f\"Procesando dataflow: {dataflow_name}\")\n",
    "\n",
    "    # --- 4. Obtener Spark y Ejecutar el Framework ---\n",
    "    \n",
    "    # utils.get_spark_session() detectará que está en Databricks\n",
    "    # y simplemente obtendrá la sesión 'spark' existente.\n",
    "    spark = utils.get_spark_session()\n",
    "    \n",
    "    # ¡Aquí es donde se llama a todo tu código de 'src' (orchestrator, writers, etc.)!\n",
    "    orchestrator.run_single_dataflow(spark, dataflow_config)\n",
    "    \n",
    "    log.info(f\"Dataflow {dataflow_name} completado exitosamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error fatal durante la ejecución del dataflow: {e}\", exc_info=True)\n",
    "    # Lanzamos el error para que la Tarea del Job de Databricks falle\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
